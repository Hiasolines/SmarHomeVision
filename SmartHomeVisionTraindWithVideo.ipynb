{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9edcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import time\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definition\n",
    "res_x = 1280\n",
    "res_y = 720\n",
    "\n",
    "labels = []\n",
    "training_folder = ''\n",
    "\n",
    "url_cam = \"http://admin:12345@10.100.91.200/image/jpeg.cgi\"\n",
    "url_shelly = \"http://10.100.91.43:8080/rest/items/ShellyLight_Betrieb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816926fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# hands, pose, face landmarks\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "# Data storage: dict mapping label -> list of video sequences\n",
    "# Each sequence is shape (num_frames, total_landmarks)\n",
    "sequences_by_label = defaultdict(list)\n",
    "\n",
    "def extract_landmarks_from_frame(holistic_results):\n",
    "    landmarks = []\n",
    "    \n",
    "    # Left hand landmarks \n",
    "    if holistic_results.left_hand_landmarks:\n",
    "        for lm in holistic_results.left_hand_landmarks.landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        landmarks.extend([0.0] * 63) \n",
    "    \n",
    "    # Right hand landmarks\n",
    "    if holistic_results.right_hand_landmarks:\n",
    "        for lm in holistic_results.right_hand_landmarks.landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        landmarks.extend([0.0] * 63)\n",
    "    \n",
    "    # Pose landmarks\n",
    "    if holistic_results.pose_landmarks:\n",
    "        for lm in holistic_results.pose_landmarks.landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        landmarks.extend([0.0] * 99)\n",
    "    \n",
    "    # Face landmarks\n",
    "    if holistic_results.face_landmarks:\n",
    "        for lm in holistic_results.face_landmarks.landmark:\n",
    "            landmarks.extend([lm.x, lm.y, lm.z])\n",
    "    else:\n",
    "        landmarks.extend([0.0] * 1404)\n",
    "    \n",
    "    return np.array(landmarks, dtype=np.float32)\n",
    "\n",
    "\n",
    "def process_video(video_path, label_idx):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    frame_landmarks = []\n",
    "    \n",
    "    with mp_holistic.Holistic(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Convert BGR to RGB\n",
    "            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image_rgb.flags.writeable = False\n",
    "            \n",
    "            # Process frame with holistic model\n",
    "            results = holistic.process(image_rgb)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            landmarks = extract_landmarks_from_frame(results)\n",
    "            frame_landmarks.append(landmarks)\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "    \n",
    "    if frame_count == 0:\n",
    "        print(f\"Warning: No frames processed from {video_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Convert list to numpy array: shape (num_frames, num_landmarks)\n",
    "    sequence = np.array(frame_landmarks, dtype=np.float32)\n",
    "    print(f\"Processed: {video_path} - Shape: {sequence.shape} - Label: {labels[label_idx]}\")\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "# Process all videos in training folder\n",
    "print(\"Processing videos...\\n\")\n",
    "for label in labels:\n",
    "    label_idx = labels.index(label)\n",
    "    label_folder = os.path.join(training_folder, label)\n",
    "    \n",
    "    if not os.path.isdir(label_folder):\n",
    "        print(f\"Warning: Folder not found {label_folder}\")\n",
    "        continue\n",
    "    \n",
    "    for video_file in os.listdir(label_folder):\n",
    "        if video_file.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "            video_path = os.path.join(label_folder, video_file)\n",
    "            try:\n",
    "                sequence = process_video(video_path, label_idx)\n",
    "                if sequence is not None:\n",
    "                    sequences_by_label[label_idx].append(sequence)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_path}: {e}\")\n",
    "\n",
    "print(f\"\\n=== Dataset Summary ===\")\n",
    "print(f\"Total landmarks per frame: 63 (left hand) + 63 (right hand) + 99 (pose) + 1404 (face) = 1629\")\n",
    "print(f\"\\nSequences per label:\")\n",
    "for label_idx, sequences in sorted(sequences_by_label.items()):\n",
    "    label_name = labels[label_idx]\n",
    "    print(f\"  {label_name}: {len(sequences)} videos\")\n",
    "    if sequences:\n",
    "        print(f\"    Sample sequence shape: {sequences[0].shape}\")\n",
    "\n",
    "# Create lists for model training\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "\n",
    "for label_idx in sorted(sequences_by_label.keys()):\n",
    "    for sequence in sequences_by_label[label_idx]:\n",
    "        all_sequences.append(sequence)\n",
    "        all_labels.append(label_idx)\n",
    "\n",
    "print(f\"\\nTotal sequences for training: {len(all_sequences)}\")\n",
    "print(f\"Label distribution: {dict(zip([labels[i] for i in sorted(sequences_by_label.keys())], \n",
    "                                       [len(sequences_by_label[i]) for i in sorted(sequences_by_label.keys())]))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26658c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]\n",
      "Python executable: c:\\Users\\matth\\.pyenv\\pyenv-win\\versions\\3.10.9\\python.exe\n",
      "Requirement already satisfied: pip in c:\\users\\matth\\.pyenv\\pyenv-win\\versions\\3.10.9\\lib\\site-packages (25.3)\n",
      "Requirement already satisfied: pip in c:\\users\\matth\\.pyenv\\pyenv-win\\versions\\3.10.9\\lib\\site-packages (25.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow.keras.models (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow.keras.models\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8269bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_length = max(len(seq) for seq in all_sequences)\n",
    "padded_sequences = np.array([np.pad(seq, ((0, max_length - len(seq)), (0, 0)), mode='constant') for seq in all_sequences])\n",
    "\n",
    "# Convert labels to categorical\n",
    "num_classes = len(labels)\n",
    "y = to_categorical(all_labels, num_classes=num_classes)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(max_length, 1629)),\n",
    "    Dropout(0.5),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate on test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
