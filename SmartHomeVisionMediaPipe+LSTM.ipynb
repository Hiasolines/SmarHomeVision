{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90cb6d04",
   "metadata": {},
   "source": [
    "# Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27700669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import time\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Definition\n",
    "res_x = 1280\n",
    "res_y = 720\n",
    "#labels = [\"closedFist\", \"DaumenUP\", \"fingerCircle\", \"fingerSymbols\", \"none\", \"openPalm\", \"point\"]\n",
    "labels = [\"paper\", \"rock\", \"scissors\"]\n",
    "training_folder = 'training_images_rps'\n",
    "\n",
    "url_cam = \"http://admin:12345@10.100.91.200/image/jpeg.cgi\"\n",
    "url_shelly = \"http://10.100.91.43:8080/rest/items/ShellyLight_Betrieb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install mediapipe pandas tensorflow scikit-learn matplotlib opencv-python pillow requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25d3ef",
   "metadata": {},
   "source": [
    "# Building Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_set = []\n",
    "image_set_labels = []\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Landmark normalization and augmentation functions\n",
    "def normalize_landmarks(landmarks_array):\n",
    "    \"\"\"Normalize landmarks to be scale and translation invariant\"\"\"\n",
    "    landmarks = landmarks_array.reshape(-1, 3)\n",
    "\n",
    "    wrist = landmarks[0].copy()\n",
    "\n",
    "    landmarks = landmarks - wrist\n",
    "\n",
    "    distances = np.linalg.norm(landmarks, axis=1)\n",
    "    max_distance = np.max(distances)\n",
    "    \n",
    "    if max_distance > 0:\n",
    "        landmarks = landmarks / max_distance\n",
    "    return landmarks.flatten()\n",
    "\n",
    "\n",
    "def rotate_landmarks_3d(landmarks_array, angle_x=0, angle_y=0, angle_z=0):\n",
    "    \"\"\"Rotate 3D landmarks around x, y, z axes\"\"\"\n",
    "    landmarks = landmarks_array.reshape(-1, 3)\n",
    "    cos_x, sin_x = np.cos(angle_x), np.sin(angle_x)\n",
    "    Rx = np.array([[1, 0, 0],\n",
    "                   [0, cos_x, -sin_x],\n",
    "                   [0, sin_x, cos_x]])\n",
    "    cos_y, sin_y = np.cos(angle_y), np.sin(angle_y)\n",
    "    Ry = np.array([[cos_y, 0, sin_y],\n",
    "                   [0, 1, 0],\n",
    "                   [-sin_y, 0, cos_y]])\n",
    "    cos_z, sin_z = np.cos(angle_z), np.sin(angle_z)\n",
    "    Rz = np.array([[cos_z, -sin_z, 0],\n",
    "                   [sin_z, cos_z, 0],\n",
    "                   [0, 0, 1]])\n",
    "    landmarks = landmarks @ Rx.T @ Ry.T @ Rz.T\n",
    "    return landmarks.flatten()\n",
    "\n",
    "\n",
    "def augment_landmarks(landmarks_array, num_augmentations=10):\n",
    "    # Default rotation ranges (radians): ~20° X/Y, ~28° Z\n",
    "    rotation_defaults = {'x': 0.35, 'y': 0.35, 'z': 0.5}\n",
    "\n",
    "    augmented = [landmarks_array]\n",
    "\n",
    "    for _ in range(num_augmentations):\n",
    "        angle_x = np.random.uniform(-rotation_defaults['x'], rotation_defaults['x'])\n",
    "        angle_y = np.random.uniform(-rotation_defaults['y'], rotation_defaults['y'])\n",
    "        angle_z = np.random.uniform(-rotation_defaults['z'], rotation_defaults['z'])\n",
    "\n",
    "        rotated = rotate_landmarks_3d(landmarks_array.copy(), angle_x, angle_y, angle_z)\n",
    "        noise = np.random.normal(0, 0.02, rotated.shape)\n",
    "        augmented_sample = rotated + noise\n",
    "\n",
    "        # Append rotated sample\n",
    "        augmented.append(augmented_sample)\n",
    "\n",
    "        # Append mirrored version (flip X)\n",
    "        mirrored = augmented_sample.copy().reshape(-1, 3)\n",
    "        mirrored[:, 0] *= -1\n",
    "        augmented.append(mirrored.flatten())\n",
    "\n",
    "    # also add mirrored original\n",
    "    mirrored_orig = landmarks_array.copy().reshape(-1, 3)\n",
    "    mirrored_orig[:, 0] *= -1\n",
    "    augmented.append(mirrored_orig.flatten())\n",
    "\n",
    "    return augmented\n",
    "\n",
    "\n",
    "def process_image_by_mediapipe(image, hands):\n",
    "    \"\"\"Process image with an existing hands instance\"\"\"\n",
    "    hand_results = hands.process(image)\n",
    "    \n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        hand_landmarks = hand_results.multi_hand_landmarks[0]\n",
    "        \n",
    "        landmarks_array = []\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            landmarks_array.extend([landmark.x, landmark.y, landmark.z])\n",
    "        \n",
    "        landmarks_array = np.array(landmarks_array)\n",
    "        \n",
    "        # Normalize the landmarks\n",
    "        landmarks_array = normalize_landmarks(landmarks_array)\n",
    "        \n",
    "        return landmarks_array\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Create hands instance once outside the loop\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.5,  # Lower threshold to catch more distant hands\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    \n",
    "    for image_type in os.listdir(training_folder):\n",
    "        if image_type in labels:\n",
    "            label_idx = labels.index(image_type)\n",
    "            for img_file in os.listdir(f'{training_folder}/{image_type}'):\n",
    "                image_path = os.path.join(training_folder, image_type, img_file)\n",
    "                image = cv2.imread(image_path)\n",
    "                if image is not None:\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    try:\n",
    "                        hand_landmarks = process_image_by_mediapipe(image, hands)\n",
    "                        if hand_landmarks is not None:\n",
    "                            # Add original sample\n",
    "                            image_set.append(hand_landmarks)\n",
    "                            image_set_labels.append(label_idx)\n",
    "                            \n",
    "                            # Add augmented samples (more rotations + mirrored versions)\n",
    "                            augmented_samples = augment_landmarks(hand_landmarks, num_augmentations=10)\n",
    "                            for aug_sample in augmented_samples[1:]:  # Skip first (original)\n",
    "                                image_set.append(aug_sample)\n",
    "                                image_set_labels.append(label_idx)\n",
    "                            \n",
    "                            print(f\"Processed: {image_path} - Generated {len(augmented_samples)} samples\")\n",
    "                        else:\n",
    "                            print(f\"No hand detected in: {image_path}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "\n",
    "# Shuffle dataset to avoid chronological order\n",
    "if len(image_set) > 1:\n",
    "    perm = np.random.permutation(len(image_set))\n",
    "    image_set = [image_set[i] for i in perm]\n",
    "    image_set_labels = [image_set_labels[i] for i in perm]\n",
    "    print(\"Shuffled dataset to remove chronological ordering.\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "image_set = np.array(image_set, dtype=np.float32)\n",
    "image_set_labels = np.array(image_set_labels, dtype=np.int32)\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {image_set.shape}\")\n",
    "print(f\"Labels shape: {image_set_labels.shape}\")\n",
    "print(f\"Each sample has {image_set.shape[1]} features (21 landmarks × 3 coordinates)\")\n",
    "print(f\"Total samples including augmentation: {len(image_set)}\")\n",
    "\n",
    "# Quick preview of label distribution\n",
    "unique, counts = np.unique(image_set_labels, return_counts=True)\n",
    "print(dict(zip([labels[u] for u in unique], counts)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import mediapipe as mp\n",
    "\n",
    "HAND_CONNECTIONS = mp.solutions.hands.HAND_CONNECTIONS  # list of (a,b) pairs\n",
    "\n",
    "def visualize_landmarks(landmarks_flat, canvas_size=(400,400), circle_r=4, line_w=2):\n",
    "    lm = np.array(landmarks_flat).reshape(-1, 3)  # (21,3)\n",
    "    h, w = canvas_size[1], canvas_size[0]\n",
    "\n",
    "    # landmarks in your notebook are wrist-centered and scaled by max_distance (range roughly -1..1).\n",
    "    # Map x,y from approx [-1,1] -> [0..w-1]/[0..h-1]\n",
    "    coords = lm[:, :2].copy()\n",
    "    coords = (coords + 1.0) * 0.5  # now ~[0..1]\n",
    "    coords[:, 0] *= w\n",
    "    coords[:, 1] *= h\n",
    "    coords = coords.astype(int)\n",
    "\n",
    "    canvas = np.ones((h, w, 3), dtype=np.uint8) * 255  # white background\n",
    "    # draw lines\n",
    "    for a, b in HAND_CONNECTIONS:\n",
    "        x1, y1 = tuple(coords[a])\n",
    "        x2, y2 = tuple(coords[b])\n",
    "        cv2.line(canvas, (x1, y1), (x2, y2), (0, 200, 0), line_w)\n",
    "    # draw points\n",
    "    for (x, y) in coords:\n",
    "        cv2.circle(canvas, (x, y), circle_r, (0, 0, 255), -1)\n",
    "\n",
    "    # Display\n",
    "    display(Image.fromarray(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "# Example: show a random sample (if image_set contains landmarks)\n",
    "\n",
    "rand = np.random.randint(0, len(image_set))\n",
    "\n",
    "visualize_landmarks(image_set[rand])\n",
    "print(f\"Label: {labels[image_set_labels[rand]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1b1b8",
   "metadata": {},
   "source": [
    "# Training and Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a4cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Class Distribution ===\")\n",
    "for i, label in enumerate(labels):\n",
    "    count = np.sum(image_set_labels == i)\n",
    "    print(f\"{label}: {count} samples ({count/len(image_set_labels)*100:.1f}%)\")\n",
    "\n",
    "def print_training_results(history, model):\n",
    "    predictions = model.predict(image_set)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(image_set_labels, predicted_labels, target_names=labels))\n",
    "    print(\"\\n=== Confusion Matrix ===\")\n",
    "    cm = confusion_matrix(image_set_labels, predicted_labels)\n",
    "    print(cm)\n",
    "\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(image_set_labels),\n",
    "    y=image_set_labels\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"\\nClass weights: {class_weight_dict}\")\n",
    "\n",
    "def build_NN_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(63,)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(len(labels), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_LSTM_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.Input(shape=(63,)),\n",
    "        tf.keras.layers.Reshape((21, 3)),\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(len(labels), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_LSTM_model()\n",
    "#model.summary()\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    image_set, \n",
    "    image_set_labels, \n",
    "    epochs=100, \n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1,\n",
    ")\n",
    "model.save('model.keras')\n",
    "\n",
    "print_training_results(history, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b050d",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5be41d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: scissors with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: scissors with confidence 0.71\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 0.99\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 0.99\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 0.97\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 0.99\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 0.52\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: scissors with confidence 0.55\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: scissors with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: scissors with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: scissors with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: rock with confidence 1.00\n",
      "✓ Light successfully set to ON\n",
      "Status Code: 200\n",
      "Response: \n",
      "Prediction: paper with confidence 1.00\n",
      "✓ Light successfully set to OFF\n",
      "Status Code: 200\n",
      "Response: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m image_rgb_processing \u001b[38;5;241m=\u001b[39m image_rgb\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    104\u001b[0m image_rgb_processing\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m hand_results \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb_processing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m landmarks \u001b[38;5;241m=\u001b[39m process_image_by_mediapipe(image_rgb_processing, hands)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m landmarks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\matth\\.pyenv\\pyenv-win\\versions\\3.10.9\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\.pyenv\\pyenv-win\\versions\\3.10.9\\lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_picture_from_url(url = url_cam):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    # PIL returns RGB, keep it as RGB for now\n",
    "    return np.array(image)\n",
    "\n",
    "def normalize_landmarks(landmarks_array):\n",
    "    \"\"\"Normalize landmarks to be scale and translation invariant\"\"\"\n",
    "    # Reshape to get x, y, z separately\n",
    "    landmarks = landmarks_array.reshape(-1, 3)\n",
    "    \n",
    "    # Get wrist (landmark 0) as reference point\n",
    "    wrist = landmarks[0].copy()\n",
    "    \n",
    "    # Translate so wrist is at origin\n",
    "    landmarks = landmarks - wrist\n",
    "    \n",
    "    # Calculate the scale (max distance from wrist)\n",
    "    distances = np.linalg.norm(landmarks, axis=1)\n",
    "    max_distance = np.max(distances)\n",
    "    \n",
    "    # Normalize by max distance to make scale-invariant\n",
    "    if max_distance > 0:\n",
    "        landmarks = landmarks / max_distance\n",
    "    \n",
    "    # Flatten back\n",
    "    return landmarks.flatten()\n",
    "\n",
    "def switch_light(predicted_label):\n",
    "    if predicted_label == \"rock\":\n",
    "        control_shelly_light(state=\"ON\")\n",
    "    elif predicted_label == \"paper\":\n",
    "        control_shelly_light(state=\"OFF\")\n",
    "    elif predicted_label == \"scissors\":\n",
    "        control_shelly_light(state=\"OFF\")\n",
    "\n",
    "def control_shelly_light(state=\"ON\", auth_token=\"oh.ToggleLight.i994VVCmkKJgzwUanlkAFP1Pi86QpajtliS9OYdETG1vBh1c58DQTnZa0mjXp95MS8KBpYn7Fu5GtYRgbiaQ\", url = url_shelly):\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"text/plain\"\n",
    "    }\n",
    "    \n",
    "    # Add auth token if provided\n",
    "    if auth_token:\n",
    "        headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n",
    "    \n",
    "    try:\n",
    "        # Send POST request with the state as data\n",
    "        response = requests.post(url, data=state, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(f\"✓ Light successfully set to {state}\")\n",
    "            print(f\"Status Code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "        else:\n",
    "            print(f\"✗ Failed to control light\")\n",
    "            print(f\"Status Code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            \n",
    "        return response\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"✗ Error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_image_by_mediapipe(image, hands):\n",
    "    hand_results = hands.process(image)\n",
    "    \n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        hand_landmarks = hand_results.multi_hand_landmarks[0]\n",
    "        landmarks_array = []\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            landmarks_array.extend([landmark.x, landmark.y, landmark.z])\n",
    "        \n",
    "        landmarks_array = np.array(landmarks_array)\n",
    "        \n",
    "        # Normalize the landmarks\n",
    "        landmarks_array = normalize_landmarks(landmarks_array)\n",
    "        \n",
    "        return landmarks_array\n",
    "    else: \n",
    "        return None\n",
    "\n",
    "list_hand_landmarks = []\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "        min_detection_confidence=0.8,\n",
    "        min_tracking_confidence=0.5)\n",
    "\n",
    "model = tf.keras.models.load_model('model.keras')\n",
    "\n",
    "display_image = False \n",
    "\n",
    "while True:\n",
    "    frame = get_picture_from_url(url_cam)  # Returns RGB image\n",
    "    image_rgb = cv2.flip(frame, 1)  # Flip horizontally, keep RGB\n",
    "    \n",
    "    # Make a copy for MediaPipe processing\n",
    "    image_rgb_processing = image_rgb.copy()\n",
    "    image_rgb_processing.flags.writeable = False\n",
    "    hand_results = hands.process(image_rgb_processing)\n",
    "\n",
    "    landmarks = process_image_by_mediapipe(image_rgb_processing, hands)\n",
    "    \n",
    "    if landmarks is None:\n",
    "        if display_image:\n",
    "            image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "            cv2.putText(image_bgr, \"No hand detected\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            cv2.imshow('MediaPipe Hands', image_bgr)\n",
    "    else:\n",
    "        prediction = model.predict(np.array([landmarks], dtype=np.float32), verbose=0)\n",
    "        predicted_label = labels[np.argmax(prediction)]\n",
    "        confidence = np.max(prediction)\n",
    "        \n",
    "        print(f\"Prediction: {predicted_label} with confidence {confidence:.2f}\")\n",
    "        switch_light(predicted_label)\n",
    "        if display_image:\n",
    "            image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "            if hand_results.multi_hand_landmarks:\n",
    "                for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image_bgr,\n",
    "                        hand_landmarks,\n",
    "                        mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                        mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2)\n",
    "                    )\n",
    "            text = f\"{predicted_label}: {confidence:.2%}\"\n",
    "            cv2.putText(image_bgr, text, (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow('MediaPipe Hands', image_bgr)\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q') & display_image:\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b602e05",
   "metadata": {},
   "source": [
    "# Just MediaPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "860b0f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: rock with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: paper with confidence 1.00\n",
      "Prediction: scissors with confidence 1.00\n",
      "--- Hand Landmarks DataFrame ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>hand_id</th>\n",
       "      <th>landmark</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.178537</td>\n",
       "      <td>0.669285</td>\n",
       "      <td>-5.491133e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.192061</td>\n",
       "      <td>0.610426</td>\n",
       "      <td>1.678064e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.227453</td>\n",
       "      <td>0.574999</td>\n",
       "      <td>2.045002e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.255540</td>\n",
       "      <td>0.569960</td>\n",
       "      <td>2.023895e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.578728</td>\n",
       "      <td>1.864945e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.722239</td>\n",
       "      <td>0.470668</td>\n",
       "      <td>-6.580956e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3776</th>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.729762</td>\n",
       "      <td>0.533465</td>\n",
       "      <td>-4.327568e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3777</th>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.770903</td>\n",
       "      <td>0.478535</td>\n",
       "      <td>-6.234528e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.765163</td>\n",
       "      <td>0.482829</td>\n",
       "      <td>-6.125655e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.747578</td>\n",
       "      <td>0.501286</td>\n",
       "      <td>-5.727903e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3780 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      frame  hand_id  landmark         x         y             z\n",
       "0         0        0         0  0.178537  0.669285 -5.491133e-07\n",
       "1         0        0         1  0.192061  0.610426  1.678064e-02\n",
       "2         0        0         2  0.227453  0.574999  2.045002e-02\n",
       "3         0        0         3  0.255540  0.569960  2.023895e-02\n",
       "4         0        0         4  0.267841  0.578728  1.864945e-02\n",
       "...     ...      ...       ...       ...       ...           ...\n",
       "3775    178        0        16  0.722239  0.470668 -6.580956e-02\n",
       "3776    178        0        17  0.729762  0.533465 -4.327568e-02\n",
       "3777    178        0        18  0.770903  0.478535 -6.234528e-02\n",
       "3778    178        0        19  0.765163  0.482829 -6.125655e-02\n",
       "3779    178        0        20  0.747578  0.501286 -5.727903e-02\n",
       "\n",
       "[3780 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_landmarks_to_dataframe(all_frames_landmarks, landmark_type='hand'):\n",
    "    \"\"\"\n",
    "    Processes a list of landmark data from all frames and converts it into a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    for frame_idx, frame_landmarks in enumerate(all_frames_landmarks):\n",
    "        if frame_landmarks:\n",
    "            for object_idx, specific_landmarks in enumerate(frame_landmarks):\n",
    "                for landmark_idx, landmark in enumerate(specific_landmarks.landmark):\n",
    "                    processed_data.append({\n",
    "                        'frame': frame_idx,\n",
    "                        f'{landmark_type}_id': object_idx,\n",
    "                        'landmark': landmark_idx,\n",
    "                        'x': landmark.x,\n",
    "                        'y': landmark.y,\n",
    "                        'z': landmark.z\n",
    "                    })\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    return df\n",
    "\n",
    "def get_picture_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "def process_image_by_mediapipe(image, hands):\n",
    "    hand_results = hands.process(image)\n",
    "    \n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        hand_landmarks = hand_results.multi_hand_landmarks[0]\n",
    "        landmarks_array = []\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            landmarks_array.extend([landmark.x, landmark.y, landmark.z])\n",
    "        \n",
    "        landmarks_array = np.array(landmarks_array)\n",
    "        \n",
    "        # Normalize the landmarks\n",
    "        landmarks_array = normalize_landmarks(landmarks_array)\n",
    "        \n",
    "        return landmarks_array\n",
    "    else: \n",
    "        return None\n",
    "    \n",
    "def normalize_landmarks(landmarks_array):\n",
    "    \"\"\"Normalize landmarks to be scale and translation invariant\"\"\"\n",
    "    # Reshape to get x, y, z separately\n",
    "    landmarks = landmarks_array.reshape(-1, 3)\n",
    "    \n",
    "    # Get wrist (landmark 0) as reference point\n",
    "    wrist = landmarks[0].copy()\n",
    "    \n",
    "    # Translate so wrist is at origin\n",
    "    landmarks = landmarks - wrist\n",
    "    \n",
    "    # Calculate the scale (max distance from wrist)\n",
    "    distances = np.linalg.norm(landmarks, axis=1)\n",
    "    max_distance = np.max(distances)\n",
    "    \n",
    "    # Normalize by max distance to make scale-invariant\n",
    "    if max_distance > 0:\n",
    "        landmarks = landmarks / max_distance\n",
    "    \n",
    "    # Flatten back\n",
    "    return landmarks.flatten()\n",
    "    \n",
    "\n",
    "\n",
    "###------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "list_hand_landmarks = []\n",
    "list_face_landmarks = []\n",
    "df_hand_landmarks = []\n",
    "\n",
    "\n",
    "# Initialize MediaPipe drawing, hands, and face_mesh utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "model = tf.keras.models.load_model('model.keras')\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define drawing styles for face mesh\n",
    "tesselation_drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=(100,150,100))\n",
    "contours_drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=(150,200,250))\n",
    "\n",
    "REFINED_LANDMARKS = False\n",
    "\n",
    "\n",
    "# Initialize Hands, Face Mesh, and Pose models\n",
    "hands = mp_hands.Hands(\n",
    "        min_detection_confidence=0.8,\n",
    "        min_tracking_confidence=0.5)\n",
    "pose = mp_pose.Pose(\n",
    "            model_complexity=1,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5)\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    #ret, frame = cap.read()\n",
    "    #if not ret:\n",
    "    #    print(\"Ignoring empty camera frame.\")\n",
    "    #    continue\n",
    "\n",
    "    frame = cap.read()[1]\n",
    "\n",
    "    # Flip the image horizontally for a selfie-view display\n",
    "    # Convert the BGR image to RGB.\n",
    "    image_rgb = cv2.cvtColor(cv2.flip(frame, 1), cv2.COLOR_BGR2RGB)\n",
    "    image_rgb.flags.writeable = False\n",
    "    \n",
    "    # Process with all models\n",
    "    hand_results = hands.process(image_rgb)\n",
    "    pose_results = pose.process(image_rgb)\n",
    "    \n",
    "    image_rgb.flags.writeable = True\n",
    "    image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR) # Convert back to BGR for OpenCV\n",
    "\n",
    "\n",
    "    # ------------- Data Capture -------------------\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        list_hand_landmarks.append(hand_results.multi_hand_landmarks)\n",
    "\n",
    "    landmarks = process_image_by_mediapipe(image_rgb, hands)\n",
    "\n",
    "\n",
    "    # ------------- Drawing -------------------\n",
    "    # Draw hand landmarks\n",
    "    if hand_results.multi_hand_landmarks:\n",
    "        for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image_bgr,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "    # Draw pose landmarks\n",
    "    if pose_results.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_bgr,\n",
    "            pose_results.pose_landmarks,\n",
    "            mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
    "        )\n",
    "\n",
    "\n",
    "    if landmarks is None:\n",
    "        # Display \"No hand detected\" on the image\n",
    "        cv2.putText(image_bgr, \"No hand detected\", (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        prediction = model.predict(np.array([landmarks], dtype=np.float32), verbose=0)\n",
    "        predicted_label = labels[np.argmax(prediction)]\n",
    "        confidence = np.max(prediction)\n",
    "        \n",
    "        # Display prediction on the image\n",
    "        text = f\"{predicted_label}: {confidence:.2%}\"\n",
    "        cv2.putText(image_bgr, text, (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        print(f\"Prediction: {predicted_label} with confidence {confidence:.2f}\") \n",
    "\n",
    "    # ---------------- End ----------------------\n",
    "    cv2.imshow('Body Tracking', image_bgr)\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    delay = 0.1  # seconds\n",
    "    time.sleep(delay)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# --- Convert the captured landmark data to DataFrames ---\n",
    "hand_df = process_landmarks_to_dataframe(list_hand_landmarks, 'hand')\n",
    "\n",
    "\n",
    "# --- Display the DataFrames ---\n",
    "print(\"--- Hand Landmarks DataFrame ---\")\n",
    "# Using display() is great for notebooks like Jupyter/Dataspell.\n",
    "# If running a standard .py script, you might prefer print(hand_df)\n",
    "display(hand_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "url = \"http://admin:12345@10.100.91.200/image/jpeg.cgi\"\n",
    "\n",
    "print(\"Press 'q' to quit...\")\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=5)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        image_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow(\"Camera Feed\", image_bgr)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching image: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        print(\"Quitting...\")\n",
    "        break\n",
    "    \n",
    "    time.sleep(0.01)  # Small delay to prevent busy waiting\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
